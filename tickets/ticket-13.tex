\section{Теория вероятностей} 


\subsection{Вероятность}

Наивное определение вероятности события А:

$$
P(A) = \frac{N_A}{N}
$$

где $N_A$ --- число появлений события $A$, $N$ --- число событий

\subsection{Условная вероятность}

\textbf{Условная вероятность} --- вероятность события, когда уже что-то случилось.
Обозначается $P(A|B)$, где $P(A \mid B)$ --- вероятность события $A$ при условии, что $B$ сбылось.

$$
P(A \mid B)=\frac{P(A \cap B)}{P(B)}
$$

События $A$ и $B$ называются независимыми, если $P(A \cap B) \hm= P(A) P(B)$
 или что то же самое $P(A) = P(A \mid B)$
 
\subsection{Формула Байеса}

Знаем, что $P(A \mid B)=\frac{P(A \cap B)}{P(B)}$. Найдём вероятность $P(B \mid A)$. 
По формуле условной вероятности:
$$
P(B \mid A)=\frac{P(A \cap B)}{P(A)}
$$

Так как $P(A \cap B)$ --- общее в этих формулах, из формулы условной вероятности $P(A \cap B) = P (A \mid B) P(B)$. Подставим:

$$
P(B \mid A)=\frac{P (A \mid B) P(B)}{P(A)}
$$

Это и есть \textbf{формула Байеса}, которая позволяет "переставить причину и следствие", то есть найти вероятность того, что событие $B$ вызвано причиной $A$.

\subsection{Формула полной вероятности}

Пусть $B_1, B_2, ... , B_n$ --- несовместные события, то есть те, которые не могут произойти одновременно. Кроме того, пусть
$$P(B_1) + P(B_2) + ... + P(B_n) \hm= 1$$
То есть хотя бы одно из событий обязательно произойдёт. В таком случае говорят, что $B_1, B_2, ... , B_n$ образуют полную группу несовместных событий.

Пусть ещё есть событие $A$. 

Рассмотрим вероятности $P(A \cup B_1), P(A \cup B_2), ... P(A \cup B_n)$.
$$
P(A) = P(A \cup B_1) + P(A \cup B_2)+ ... + P(A \cup B_n)
$$

Распишем каждую условную вероятность по формуле условной вероятности:
$$
P(A) = P(A \mid B_1) P(B_1) + P(A \mid B_2)P(B_2)+ ... + P(A \mid B_n) P(B_n)
$$
$$
P(A) = \sum_{i=1}^{n} P(B_i)P(A \mid B_i)
$$

С помощью этого результата можно усовершенствовать формулу Байеса:
$$
P(B_i \mid A)=\frac{P (A \mid B_i) P(B_i)}{P(A)} = \frac{P (A \mid B_i) P(B_i)}{\sum_{i=1}^{n} P(B_i)P(A \mid B_i)}
$$

Вероятности $P(B_i \mid A)$ называют \textbf{апостериорными} вероятностями (то есть вероятности, которые немного уточнены, так как произошло событие $A$). А $P(B_i)$ --- \textbf{априорные} вероятности, то есть те, что известны до проведения эксперимента, связанного с $A$.


\subsection{Распределения вероятностей}

Распределение вероятностей --- это функция, которая говорит нам, какая вероятность у каждого результата эксперимента.

Например, распределение результатов выпадения игральной кости --- это равномерное распределение с вероятностью 1/6.

Результат случайного эксперимента называют \textbf{случайной величиной}. Говорят, что случайная величина подчиняется какому-то распределению вероятностей. Если $X$ --- результат подброса игральной кости, то пишут:

$$
X \sim Uniform(1/6)
$$

\subsection{ММП - Метод максимального правдоподобия}
Пусть на входе мы имеем какую-то случайную выборку чисел $X$. Нам хотелось бы определить, из какого они распределения. 

Если мы это узнаем, то можем, например, генерировать больше данных для обучения. Обычно, если построить гистограмму, то можно на глаз оценить, к какому семейству распределений оно относится (нормальное, экспоненциальное, пуассона и т.д.). 

Осталось найти параметры этих распределений, которые обозначим за $\theta $.

Так как мы знаем, к какому семейству относится наше распределение, то мы можем записать вероятность или плотность вероятности $p(x)$ , как функцию, в которой кроме $x$ будет ещё фигурировать $\theta$. 

Суть метода заключается в следующем:

\begin{enumerate}
    \item Находим вероятность нашей выборки : $$P(X \mid \theta) = \prod_{i = 1}^{n} P(x_{i} \mid \theta)$$ -- функция правдоподобия.
    \item Давайте будем искать такой $\theta$, при котором эта вероятность максимальна. Почему так делают?
\end{enumerate}

Приведём пример.

Пусть у нас есть игральная кость, в которой одна из граней утяжелена, т.е. она выпадает чаще. Пусть у вас есть $1000$ бросков этой кости, и вас просят на основе этих данных понять, какая грань утяжелена. 

Естественно, что вы ответите, что утяжелена та грань, которая чаще всего выпала. Это и есть метод максимального правдоподобия.

Параметром в нашем странном распределении является номер грани, которая утяжелена. Если больше всего выпадала грань '3', то вероятность того, что она утяжелена намного больше, чем то, что утяжелена грань '2'. 

Поэтому мы перебираем все варианты $\theta$ и находим из них ту, которая даёт максимальную вероятность:
    $$\hat{\theta} = \underset{\theta}{\operatorname{argmax}} \mbox{ } \{ \log(\prod_{i = 1}^{n} P(x_{i} \mid \theta)) \} = \underset{\theta}{\operatorname{argmax}} \mbox{ } \{ \sum_{i = 1}^{n} \log(P(x_{i} \mid \theta)) \}$$
    
Если $\theta$ — вещественный параметр, то обычно используют производную функции, чтобы найти минимум. 

Совет: можно искать не минимум произведения, а логарифм от этого минимума. Так вы будете оперировать с меньшими числами и производную суммы легче посчитать, чем производную от произведения.
\subsubsection{Связь MSE и ММП}

Рассмотрим, классическую регрессионную модель: $$y_{i}= w \cdot x_{i} + \eps, \eps \sim N(0;1)$$

Другими словами, у нас есть данные какого-то измерения, которые лежат на линии(гиперплоскости в многомерном случае) с каким-то шумом $\eps$, которые подчиняется стандартному нормальному распределению $N(0;1)$.

Давайте с помощью метода максимального правдоподобия найдем параметры $w$. Что такое $y_{i}$?. Т.к. $\eps \sim N(0, 1)$, то $y_{i} \sim N(w \cdot x_{i}, 1)$, т.к. это просто сдвиг на какое-то число.

Запишем функцию правдоподобия для одного элемента выборки :
$$P(y_{i} \mid x_{i}) = \frac{1}{\sqrt{2 \pi}}\exp\{{-\frac{(y_{i} - x_{i} \cdot w) ^ {2}}{2}}\}$$
Мы просто подставили в плотность нормального распределения наши величины. Возьмём от него натуральный логарифм.

\begin{eqnarray*}
\ln\{{(P(y_{i} \mid x_{i}, w, \eps))}\} = \ln{\{\frac{1}{\sqrt{2 \pi}}\exp\{{-\frac{(y_{i} - x_{i} \cdot w) ^ {2}}{2}}\}\}}=
\\
-\frac{\ln{(2 \cdot \pi)}}{2} - \frac{1}{2}(y_{i} - x_{i} \cdot w)^2
\end{eqnarray*}

Теперь запишем логарифм функции правдоподобия для всей выборки :

$$\ln P(Y \mid X, w, \eps) = -\frac{n}{2} \cdot \ln{(2 \cdot \pi)} - \sum_{i = 1}^{n}(y_{i} - x_{i} \cdot w)^{2} \cdot \frac{1}{2}$$

Первое слагаемое можно отбросить так как оно константа, а второе слагаемое -- MSE.

Т.е когда мы минимизируем ошибку MSE, то получившиеся веса $w$ будут также являться и оценкой максимального правдоподобия, и будут обладать всеми свойствами этой оценки.

\subsection{Наивный байесовский классификатор}

Воспользуемся предыдущими формулами для задачи классификации.

Пусть у нас есть задача классификации текстов на $C=\left\{c_{1}, \ldots, c_{n}\right\}$ классов. 

В качестве признаков возьмём какие слова встречаются в текстах. Т.е. пусть во всех текстах всего $N$ уникальных слов. Мы дадим каждому слову номер от 1 до $N$. 

Признаками каждого текста будет кол-во каждого слова, которое есть в тексте. Например, 2 слова <<дом>>, 3 слова <<кот>>, 0 слов <<аниме>>. 

Таким образом, каждый текст мы можем представить в виде вектора длины $N:\left(w_{1}, \ldots, w_{N}\right)$ где $w_{i}-$ количество слов под номером $i$ в нашем тексте.

У нас есть много таких текстов, поэтому мы можем вычислить различные вероятности. 

Например:

\begin{itemize}
    \item $P(c)$ -- Сколько есть текстов каждого класса 
    
    \item $P\left(w_{1}\right)$  -- Какова вероятность встречи слова под номером 1: 
    
    \item $P\left(w_{2} \mid c\right)$ -- Сколько раз встречается слово под номером 2 в разных классах . 
    
    Для этого берём все тексты, содержащие слово $w_{2} .$ Смотрим, сколько из этих писем помечено классом $c_{1}$,
    сколько классом $c_{2}$ и т.д. 
\end{itemize}



В задаче же классификации у нас есть текст, состоящее из таких-то слов, т.е. имеет признаки. Наша задача состоит в том, чтобы понять класс текста $c$. 

Т.е. найти вероятности $P\left(c \mid w_{1}, \ldots, w_{N}\right)$,
перебрать все $c$ и выбрать из них тот класс, который даёт максимальную вероятность.

Формально:
$$\hat{c}=\operatorname{argmax} P\left(c \mid w_{1}, \ldots w_{N}\right)$$
где $\hat{c}$- наше предсказание.

Преобразуем эту вероятность по формуле Байеса:

$$P\left(c \mid w_{1}, \ldots, w_{N}\right)=\frac{P\left(c, w_{1}, \ldots w_{N}\right)}{P\left(w_{1}, \ldots, w_{N}\right)}$$

Т.к. мы максимизируем по $c$, а вероятности $w_{1}, \ldots w_{N}$ остаются
для письма теми же, то их можно убрать из argmax. 
$$P\left(c, w_{1}, \ldots, w_{N}\right)=P(c) P\left(w_{1}, \ldots, w_{N} \mid c\right)$$
Вероятность справа достаточно сложная. Давайте сделаем наивное  предположение: все слова в текстах появляются независимо друг от друга. 

Т.е. если в тексте есть 2 слова "котик", то количество "собак" может быть любым. В этом и заключается слово "наивный" в названии
данного алгоритма.
$$P\left(c, w_{1}, \ldots, w_{N}\right)=P(c) P\left(w_{1} \mid c\right) \ldots P\left(w_{N} \mid c\right)$$

Все вероятности справа мы можем подсчитать по обучающей выборке. Тогда получаем итоговый ответ алгоритма:

$$\hat{c}=\underset{c}{\operatorname{argmax}} P(c) P\left(w_{1} \mid c\right) \ldots P\left(w_{N} \mid c\right)$$
