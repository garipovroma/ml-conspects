\section{Обработка текстов}
Токенизация, Лемматизация, CountVectorizer, OneHotVectorizer, Стемминг. Word2vec (идея, какие полезные свойства эмбеддингов даёт)

\subsection{Стоп-слова}
Хотим уменьшить словарь.
Плохие слова:
\begin {itemize}
    \item Слишком частые
    \item русский язык: и, но, я, ты, ...
    \item английский язык: a, the, I, ...
    \item специфичные для коллекции: "сообщать" в новостях
    \item Слишком редкие
    \item Предлоги, междометия, частицы, цифры
\end {itemize}

\subsection{Токенизация}
\textbf{Токенизация} -- это разделение текста на токены, элементарные единицы. 
\\
В ноутбуках были примеры токенизации просто разбиением текста по пробелам, по регулярке, чтобы объединять целиком слова или имена с фамилиями, которые считаются одним словом, или по регулярке, чтобы токеном было целое предложение.

\subsection{Стемминг}
\textbf{Стемминг} - нормализация слов путем отбрасывания окончаний (согласно правилам, основанным на грамматике языка).
\\
Стеммеры(nltk):
\begin{itemize}
    \item Porter stemmer
    \item Snowball stemmer
    \item Lancaster stemmer
    \item MyStem
\end{itemize}

В целом :
\begin{itemize}
    \item Плохо работает для русского языка
    \item Нормально работает для английского
    \item Повышает качество модели
\end{itemize}

\subsection{Лемматизация}
\textbf{Лемматизация} - приведение слов к начальной морфологической форме (с помощью словаря и грамматики языка).
\\
Лемматизаторы:
\begin{itemize}
    \item pymorphy2 (язык русский, украинский)
    \item mystem3 (язык русский)
    \item Wordnet Lemmatizer (NLTK, язык английский, требует POS метку)
    \item Metaphraz (язык русский)
    \item Coda/Cadenza (языки русский и английский)
\end{itemize}

Лемматизатор на самом деле довольно сложно устроены, им нужны теги частей речи (POS).

По умолчанию функция WordNetLemmatizer.lemmatize () будет считать, что это слово является существительным, если на входе не обнаружен тег POS.

Сначала вам понадобится функция pos\_tag, чтобы пометить предложение и использовать тег, чтобы преобразовать его в теги WordNet, а затем передать его в WordNetLemmatizer.

Примечание. Лемматизация не будет работать только на одиночных словах без контекста или знании своего тега POS


В целом:

\begin{itemize}
    \item Лучше стемминга для русского языка
    \item Хорошо работает для английского языка
    \item Повышает качество модели
    \item Гораздо медленнее чем стемминг
\end{itemize}

\subsection{One-hot encoding}
Представление словаря в виде бинарных векторов, у которых все значения равны 0, кроме одного, отвечающего за соответствующее слово

\subsection{CountVectorizer}
Представление словаря в виде векторов, у которых $i$-ая координата -- количество вхождений $i$-ого слова в предложение соотвествующее этому вектору.

Может возникнуть проблема, что какое-то незначительное для классификации предложения слово может встречаться очень часто и весить очень много, хотя особого смысла в себе не несёт.

\subsection{TF-IDF Vectorizer}
Для того чтобы решить эту проблему есть TF-IDF Vectorizer.

$$tf(t, d) = \frac{n_{t}}{\sum_{k} n_{k}}$$
Где $n_{t}$ -- число вхождений слова $t$ в документ $d$, а в знаменателе -- общее число слов в документе.
$$idf(t, D) = \log{\frac{|D|}{|\{d_{i} \in D | t \in d_{i}\}|}}$$
Где $|D|$ -- число документов в коллекции, $|\{d_{i} \in D | t \in d_{i}\}|$ -- число документов из коллекции $D$, в которых встречается $t$(когда $n_{t} \neq 0$)

$$tfidf(t, d, D) = tf(t, d) \times idf(t, D)$$

Большой вес в TF-IDF получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах.

\subsection{Word2Vec}
Word2Vec - это нейросетевая модель. 

Результатом работы данной модели является словарь эмбеддингов, позволяющий сопоставлять словам их векторные представления. 

Используя такое представление, можно по какому-то новому слову, найти несколько ближайших к нему слов(имеется в виду несколько ближайших вектороных представлений других слов).

Плюс такого подхода в том, что в нем похожим по значению словам соответствуют похожие вектора - т.е. близкие друг к другу точки в $n$-мерном пространстве.

У эмбеддингов, получаемых с помощью Word2Vec есть еще одна интересная особенность - если мы можем сказать, что `А относится к B, как С к D`, то вектора слов `A - B` и `C - D` будут довольно похожи. Это значит, что если мы рассмотрим вектор `A - B + D`, то вектор `C` часто будет оказываться среди его ближайших соседей: 

$'Paris' - 'France' + 'Germany' = 'Berlin$, 

$'king' - 'man' + 'woman' = 'queen`$

Как мы видели выше, модели Word2Vec дают нам хороший способ представлять слова в виде векторов. Но для решения задач ML нам нужно научиться представлять в виде векторов тексты, а не отдельные слова. 

Построенные на основе Word2Vec эмбеддинги текстов будут сохранять их смысл и при этом их размерность будет гораздо меньше, чем у векторных представлений, получаемых с помощью tf-idf. Такие эмбеддинги бывают очень полезны в разных задачах ML.

Попробуем считать вектор документа, как среднее векторов слов из этого документа. 

Оказывается, что усреднять слова в документе - не лучший способ получить его векторное представление. Есть много слов, которые встречаются часто и есть почти во всех документах. При таком способе подсчета они будут делать вектора документов слишком похожими друг на друга.

Значит, нам нужно давать словам разные веса, когда мы вычисляем вектор документа. Большой вес должен быть у слов, которые часто встречаются в этом документе, но редко в других.

Но это как раз \textbf{tf-idf}. (Также можно использовать просто idf. TF - частота слова в документе - итак будет учтена, т.к. мы суммируем вектора слов в документе).

Осталось понять, как посчитать такую взвешенную сумму векторов. Для этого можно составить матрицу из векторов слов, встречающихся в нашей коллекции (они обязательно должны идти в том же порядке, что и в матрице из `TfidfVectorizer` - но там слова будут столбцами, а тут - строками), после чего перемножить 2 матрицы.

Пусть $DOCS$ -- число документов в коллекции, $WORDS$ -- число уникальных слов, $DIM$ -- размерность пространства эмбеддингов из Word2Vec.

Тогда первая матрица будет размера $(DOCS, WORDS)$, 

вторая - $(WORDS, DIM)$, 

а их произведение - $(DOCS, DIM)$ - это и будет матрица векторных представлений для каждого документа в коллекции. Размерность матрицы понижена, без использования $PCA$ и других алгоритмов.
